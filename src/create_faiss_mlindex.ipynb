{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a FAISS based Vector Index for Document Retrieval with AzureML\n",
    "\n",
    "We'll walk through setting up an AzureML Pipeline which pulls a Git Repo, processes the data into chunks, embeds the chunks, and creates a FAISS Vector Index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get client for AzureML Workspace\n",
    "\n",
    "The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to your workspace you created, in the main.bicep script, in which the job will be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MLClient` is how you interact with AzureML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /workspaces/rai-prompt-flow-workshop/config.json\n",
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7f3d32b187f0>,\n",
      "         subscription_id=7cfa902c-3eba-40b9-9d87-83369ec9e86d,\n",
      "         resource_group_name=labuser202_rg,\n",
      "         workspace_name=azmli)\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azureml.core import Workspace\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient.from_config(credential=credential)\n",
    "\n",
    "ws = Workspace(\n",
    "    subscription_id=ml_client.subscription_id,\n",
    "    resource_group=ml_client.resource_group_name,\n",
    "    workspace_name=ml_client.workspace_name,\n",
    ")\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Datasource?\n",
    "\n",
    "We'll be using the Contoso Dental dataset, which is a collection questions and answers from the contoso dental practice. The dataset is available in the `data` folder of this repo. *Supported File Types: .md, .txt, .html, .htm, .py, .doc, .docx, .ppt, .pptx, .pdf, .xls, .xlsx. Any other file types will be ignored during creation.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local_path = \"../data/contoso-dental.xls\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use AzureML's Data to create a dataset, which is a reference to the .xls data in the Datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading contoso-dental.xls\u001b[32m (< 1 MB): 100%|██████████| 165k/165k [00:00<00:00, 259kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data({'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'contoso-dental-clinic', 'description': 'Dental Clinic data', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourceGroups/labuser202_rg/providers/Microsoft.MachineLearningServices/workspaces/azmli/data/contoso-dental-clinic/versions/1', 'Resource__source_path': None, 'base_path': '/workspaces/rai-prompt-flow-workshop/src', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f3d3065ee80>, 'serialize': <msrest.serialization.Serializer object at 0x7f3d305dd550>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourcegroups/labuser202_rg/workspaces/azmli/datastores/workspaceblobstore/paths/LocalUpload/6516c59437bcbd1c72edb15af13f23e2/contoso-dental.xls', 'datastore': None})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "v1=\"initial\"\n",
    "\n",
    "my_data = Data(\n",
    "    name=\"contoso-dental-clinic\",\n",
    "    description=\"Dental Clinic data\",\n",
    "    path=local_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test datast to be using in evaluation of the model. *Support file types: Select a .jsonl .csv, or .tsv file, or a folder containing these file types.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_test_path = \"../data/contoso-dental.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading contoso-dental.csv\u001b[32m (< 1 MB): 100%|██████████| 124k/124k [00:00<00:00, 193kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data({'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'test-contoso-dental-dataset', 'description': 'Dental Clinic test dataset', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourceGroups/labuser202_rg/providers/Microsoft.MachineLearningServices/workspaces/azmli/data/test-contoso-dental-dataset/versions/1', 'Resource__source_path': None, 'base_path': '/workspaces/rai-prompt-flow-workshop/src', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f3d30589100>, 'serialize': <msrest.serialization.Serializer object at 0x7f3d30589df0>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourcegroups/labuser202_rg/workspaces/azmli/datastores/workspaceblobstore/paths/LocalUpload/003f723a7b73d7391fad6b21f1028763/contoso-dental.csv', 'datastore': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1=\"initial\"\n",
    "\n",
    "test_data = Data(\n",
    "    name=\"test-contoso-dental-dataset\",\n",
    "    description=\"Dental Clinic test dataset\",\n",
    "    path=local_test_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use resource group to generate unique compute name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# generating random strings to get unique compute name\n",
    "rand_val = ''.join(random.choices(string.ascii_lowercase +\n",
    "                             string.digits, k=5))\n",
    "compute_name = str('prompt-compute-') + rand_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a compute instance for prompt flow time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiated compute creation\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "from azure.ai.ml.entities import ComputeInstance\n",
    "\n",
    "all_compute_names = [x.name for x in ml_client.compute.list()]\n",
    "\n",
    "if compute_name in all_compute_names:\n",
    "    print(f\"Found existing compute: {compute_name}\")\n",
    "else:\n",
    "    my_compute = ComputeInstance(\n",
    "        name=compute_name,\n",
    "        size=\"Standard_D2_v2\",\n",
    "        idle_time_before_shutdown_minutes=60\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(my_compute)\n",
    "    print(\"Initiated compute creation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Embeddings Model to use?\n",
    "\n",
    "We will be using Azure OpenAI's `text-embedding-ada-002` embedding model to supported Embedding the dataset . Here are some factors that might influence your decision:\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "OpenAI has [great documentation](https://platform.openai.com/docs/guides/embeddings) on their Embeddings model `text-embedding-ada-002`, it can handle up to 8191 tokens and can be accessed using [Azure OpenAI](https://learn.microsoft.com/azure/cognitive-services/openai/concepts/models#embeddings-models) or OpenAI directly.\n",
    "If you have an existing **Azure OpenAI** Instance you can connect it to AzureML. The main limitation when using `text-embedding-ada-002` is cost/quota available for the model. Otherwise it provides high quality embeddings across a wide array of text domains while being simple to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can use the automatically created `Default_AzureOpenAI` connection.\n",
    "\n",
    "If you would rather use an existing Azure OpenAI connection then change `aoai_connection_name` below.\n",
    "If you would rather use an existing Azure OpenAI resource, but don't have a connection created, modify `aoai_connection_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "aoai_connection_name = \"azure-openai-conn\"\n",
    "aoai_connection = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.rag.utils.connections import (\n",
    "    get_connection_by_name_v2,\n",
    "    create_connection_v2,\n",
    ")\n",
    "\n",
    "try:\n",
    "    aoai_connection = get_connection_by_name_v2(ws, aoai_connection_name)\n",
    "except Exception as ex:\n",
    "    # Create New Connection\n",
    "    # Modify the details below to match the `Endpoint` and API key of your AOAI resource, these details can be found in Azure Portal\n",
    "\n",
    "    target = os.environ[\"AZURE_OPENAI_ENDPOINT\"]  # example: 'https://<endpoint>.openai.azure.com/'\n",
    "    key = os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "    apiVersion = \"2023-10-01-preview\"\n",
    "\n",
    "    aoai_connection = create_connection_v2(\n",
    "        workspace=ws,\n",
    "        name=aoai_connection_name,\n",
    "        category=\"AzureOpenAI\",\n",
    "        target=target,\n",
    "        auth_type=\"ApiKey\",\n",
    "        credentials={\"key\": key},\n",
    "        metadata={\"ApiType\": \"azure\", \"ApiVersion\": apiVersion},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your Workspace has a connection to Azure OpenAI we will make sure the `text-embedding-ada-002` model has been deployed ready for inference. We will be using the `text-embedding-ada-002` model your created earlier with the main.bicep script.\n",
    "\n",
    "This cell will fail if there is not deployment for the embeddings model, [follow these instructions](https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#deploy-a-model) to deploy a model with Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please create a deployment for this model by following the deploy instructions on the resource page for 'https://az-didi-openai.openai.azure.com/' in Azure Portal.\n",
      "Please create a deployment for this model by following the deploy instructions on the resource page for 'https://az-didi-openai.openai.azure.com/' in Azure Portal.\n"
     ]
    }
   ],
   "source": [
    "from azureml.rag.utils.deployment import infer_deployment\n",
    "\n",
    "aoai_embedding_model_name = \"text-embedding-ada-002\"\n",
    "oai_completion_model_name = \"gpt-35-turbo\"\n",
    "\n",
    "\n",
    "try:\n",
    "    aoai_embedding_deployment_name = infer_deployment(\n",
    "        aoai_connection, aoai_embedding_model_name\n",
    "    )\n",
    "    print(\n",
    "        f\"Deployment name in AOAI workspace for model '{aoai_embedding_model_name}' is '{aoai_embedding_deployment_name}'\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"Please create a deployment for this model by following the deploy instructions on the resource page for '{aoai_connection['properties']['target']}' in Azure Portal.\"\n",
    "    )\n",
    "    if \"ResourceId\" in aoai_connection[\"properties\"][\"metadata\"]:\n",
    "        aoai_resource_url = f\"https://portal.azure.com/resource/{aoai_connection['properties']['metadata']['ResourceId']}/overview\"\n",
    "        print(\n",
    "            f\"Please create a deployment for this model by following the deploy instructions on the resource page: {aoai_resource_url}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Please create a deployment for this model by following the deploy instructions on the resource page for '{aoai_connection['properties']['target']}' in Azure Portal.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will combine the deployment and model information into a uri form which the AzureML embeddings components expect as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_embedding_deployment_name = os.environ[\"TEXT_EMBEDDING_DEPLOYMENT_NAME\"]\n",
    "\n",
    "embeddings_model_uri = f\"azure_open_ai://deployment/{aoai_embedding_deployment_name}/model/{aoai_embedding_deployment_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azure_open_ai://deployment/text-embedding-ada-002/model/text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_model_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Pipeline to process data into Index\n",
    "\n",
    "AzureML [Pipelines](https://learn.microsoft.com/azure/machine-learning/concept-ml-pipelines?view=azureml-api-2) connect together multiple [Components](https://learn.microsoft.com/azure/machine-learning/concept-component?view=azureml-api-2). Each Component defines inputs, code that consumes the inputs and outputs produced from the code. To process your data for embedding and indexing we will chain together multiple components each performing their own step of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_registry = MLClient(credential=credential, registry_name=\"azureml\")\n",
    "\n",
    "# Clones git repository to output folder of pipeline, by default this will be on the default Workspace Datastore `workspaceblobstore`\n",
    "git_clone_component = ml_registry.components.get(\"llm_rag_git_clone\", label=\"latest\")\n",
    "# Walks input folder according to provided glob pattern (all files by default: '**/*') and attempts to open them, extract text chunks and further chunk if necessary to fir within provided `chunk_size`.\n",
    "crack_and_chunk_component = ml_registry.components.get(\n",
    "    \"llm_rag_crack_and_chunk\", label=\"latest\"\n",
    ")\n",
    "# Reads input folder of files containing chunks and their metadata as batches, in parallel, and generates embeddings for each chunk. Output format is produced and loaded by `azureml.rag.embeddings.EmbeddingContainer`.\n",
    "generate_embeddings_component = ml_registry.components.get(\n",
    "    \"llm_rag_generate_embeddings\", label=\"latest\"\n",
    ")\n",
    "# Reads input folder produced by `azureml.rag.embeddings.EmbeddingsContainer.save()` and inserts all documents (chunk, metadata, embedding_vector) int a Faiss index and in-memory document store. Writes an MLIndex yaml detailing the index and embeddings model information.\n",
    "create_faiss_index_component = ml_registry.components.get(\n",
    "    \"llm_rag_create_faiss_index\", label=\"latest\"\n",
    ")\n",
    "# Takes a uri to a storage location where an MLIndex yaml is stored and registers it as an MLIndex Data asset in the AzureML Workspace.\n",
    "register_mlindex_component = ml_registry.components.get(\n",
    "    \"llm_rag_register_mlindex_asset\", label=\"latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Component has documentation which provides an overall description of the Components purpose and each of the inputs/outputs.\n",
    "For example we can see understand what `crack_and_chunk` does by inspecting the Component definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "name: llm_rag_crack_and_chunk\n",
      "version: 0.0.55\n",
      "display_name: LLM - Crack and Chunk Data\n",
      "description: 'Creates chunks no larger than `chunk_size` from `input_data`, extracted\n",
      "  document titles are prepended to each chunk\n",
      "\n",
      "\n",
      "  LLM models have token limits for the prompts passed to them, this is a limiting\n",
      "  factor at embedding time and even more limiting at prompt completion time as only\n",
      "  so much context can be passed along with instructions to the LLM and user queries.\n",
      "\n",
      "  Chunking allows splitting source data of various formats into small but coherent\n",
      "  snippets of information which can be ''packed'' into LLM prompts when asking for\n",
      "  answers to user query related to the source documents.\n",
      "\n",
      "\n",
      "  Supported formats: md, txt, html/htm, pdf, ppt(x), doc(x), xls(x), py\n",
      "\n",
      "  '\n",
      "tags:\n",
      "  Preview: ''\n",
      "type: command\n",
      "inputs:\n",
      "  input_data:\n",
      "    type: uri_folder\n",
      "    description: Uri Folder containing files to be chunked.\n",
      "    optional: false\n",
      "  input_glob:\n",
      "    type: string\n",
      "    optional: true\n",
      "    description: Limit files opened from `input_data`, defaults to '**/*'.\n",
      "  allowed_extensions:\n",
      "    type: string\n",
      "    optional: true\n",
      "    description: Comma separated list of extensions to include, if not provided the\n",
      "      default list of supported extensions will be used. e.g. '.md,.txt,.html,.py,.pdf.'\n",
      "  chunk_size:\n",
      "    type: integer\n",
      "    optional: false\n",
      "    default: '768'\n",
      "    description: Maximum number of tokens to put in each chunk.\n",
      "  chunk_overlap:\n",
      "    type: integer\n",
      "    optional: false\n",
      "    default: '0'\n",
      "    description: Number of tokens to overlap between chunks.\n",
      "  doc_intel_connection_id:\n",
      "    type: string\n",
      "    optional: true\n",
      "    description: Connection id for Document Intelligence service. If provided, will\n",
      "      be used to extract content from .pdf document.\n",
      "  data_source_url:\n",
      "    type: string\n",
      "    optional: true\n",
      "    description: Base URL to join with file paths to create full source file URL for\n",
      "      chunk metadata.\n",
      "  document_path_replacement_regex:\n",
      "    type: string\n",
      "    optional: true\n",
      "    description: 'A JSON string with two fields, ''match_pattern'' and ''replacement_pattern''\n",
      "      to be used with re.sub on the source url. e.g. ''{\"match_pattern\": \"(.*)/articles/(.*)(\\\\.[^.]+)$\",\n",
      "      \"replacement_pattern\": \"\\\\1/\\\\2\"}'' would remove ''/articles'' from the middle\n",
      "      of the url.'\n",
      "  max_sample_files:\n",
      "    type: integer\n",
      "    optional: false\n",
      "    default: '-1'\n",
      "    description: Number of files to chunk. Specify -1 to chunk all documents in input\n",
      "      path.\n",
      "  use_rcts:\n",
      "    type: string\n",
      "    optional: false\n",
      "    default: 'True'\n",
      "    description: Whether to use RecursiveCharacterTextSplitter to split documents\n",
      "      into chunks\n",
      "    enum:\n",
      "    - 'True'\n",
      "    - 'False'\n",
      "  output_format:\n",
      "    type: string\n",
      "    optional: false\n",
      "    default: jsonl\n",
      "    description: Format of the output chunk file\n",
      "    enum:\n",
      "    - csv\n",
      "    - jsonl\n",
      "outputs:\n",
      "  output_chunks:\n",
      "    type: uri_folder\n",
      "    description: Uri Folder containing chunks. Each chunk will be a separate file\n",
      "      in the folder\n",
      "command: python -m azureml.rag.tasks.crack_and_chunk --input_data '${{inputs.input_data}}'\n",
      "  $[[--input_glob '${{inputs.input_glob}}']] $[[--allowed_extensions ${{inputs.allowed_extensions}}]]\n",
      "  --output_chunks ${{outputs.output_chunks}} --chunk_size ${{inputs.chunk_size}} --chunk_overlap\n",
      "  ${{inputs.chunk_overlap}} $[[--doc_intel_connection_id ${{inputs.doc_intel_connection_id}}]]\n",
      "  $[[--data_source_url ${{inputs.data_source_url}}]] $[[--document_path_replacement_regex\n",
      "  '${{inputs.document_path_replacement_regex}}']] --max_sample_files ${{inputs.max_sample_files}}\n",
      "  --use_rcts '${{inputs.use_rcts}}' --output_format ${{inputs.output_format}}\n",
      "environment: azureml://registries/azureml/environments/llm-rag-embeddings/versions/42\n",
      "code: azureml://registries/azureml/codes/72f46cca-d85b-4381-9d7a-d053bfb071e8/versions/1\n",
      "resources:\n",
      "  instance_count: 1\n",
      "creation_context:\n",
      "  created_at: '2024-02-10T18:55:03.239285+00:00'\n",
      "  created_by: Microsoft\n",
      "  created_by_type: User\n",
      "  last_modified_at: '2024-02-10T18:55:03.239285+00:00'\n",
      "  last_modified_by: Microsoft\n",
      "  last_modified_by_type: User\n",
      "id: azureml://registries/azureml/components/llm_rag_crack_and_chunk/versions/0.0.55\n",
      "is_deterministic: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(crack_and_chunk_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Azure OpenAI connection id for prompt flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_connection_id = aoai_connection[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a Pipeline is built by defining a python function which chains together the above components inputs and outputs. Arguments to the function are inputs to the Pipeline itself and the return value is a dictionary defining the outputs of the Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities._job.pipeline._io import PipelineInput\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "#def use_automatic_compute(component, instance_count=1, instance_type=\"Standard_E8s_v3\"):\n",
    "def use_automatic_compute(component, instance_count=1, instance_type=\"Standard_DS12_v2\"):\n",
    "    \"\"\"Configure input `component` to use automatic compute with `instance_count` and `instance_type`.\n",
    "\n",
    "    This avoids the need to provision a compute cluster to run the component.\n",
    "    \"\"\"\n",
    "    component.set_resources(\n",
    "        instance_count=instance_count,\n",
    "        instance_type=instance_type,\n",
    "        properties={\"compute_specification\": {\"automatic\": True}},\n",
    "    )\n",
    "    return component\n",
    "\n",
    "\n",
    "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n",
    "    \"\"\"Checks if optional pipeline inputs are provided.\"\"\"\n",
    "    return input is not None and input._data is not None\n",
    "\n",
    "\n",
    "# If you have an existing compute cluster you want to use instead of automatic compute, uncomment the following line, replace `dedicated_cpu_compute` with the name of your cluster.\n",
    "# Also comment out the `component.set_resources` line in `use_automatic_compute` above and the `default_compute='serverless'` line below.\n",
    "# @pipeline(compute=dedicated_cpu_compute)\n",
    "@pipeline(default_compute=\"serverless\")\n",
    "def local_to_faiss(\n",
    "    input_data: Input,\n",
    "    embeddings_model: str,\n",
    "    asset_name: str,\n",
    "    #branch_name: str = None,\n",
    "    chunk_size: int = 1024,\n",
    "    data_source_glob: str = None,\n",
    "    data_source_url: str = None,\n",
    "    document_path_replacement_regex: str = None,\n",
    "    #git_connection_id=None,\n",
    "    aoai_connection_id=None,\n",
    "    embeddings_container=None,\n",
    "):\n",
    "    \"\"\"Pipeline to generate embeddings for a `input_data` source and create a Faiss index.\"\"\"\n",
    "\n",
    "\n",
    "    crack_and_chunk = crack_and_chunk_component(\n",
    "        input_data=input_data,\n",
    "        input_glob=data_source_glob,\n",
    "        #data_source_url=my_data.datastore,\n",
    "        document_path_replacement_regex=document_path_replacement_regex,\n",
    "    )\n",
    "    use_automatic_compute(crack_and_chunk)\n",
    "\n",
    "    generate_embeddings = generate_embeddings_component(\n",
    "        chunks_source=crack_and_chunk.outputs.output_chunks,\n",
    "        embeddings_model=embeddings_model_uri, \n",
    "    )\n",
    "    use_automatic_compute(generate_embeddings)\n",
    "    if optional_pipeline_input_provided(aoai_connection_id):\n",
    "        generate_embeddings.environment_variables[\n",
    "            \"AZUREML_WORKSPACE_CONNECTION_ID_AOAI\"\n",
    "        ] = aoai_connection_id\n",
    "    if optional_pipeline_input_provided(embeddings_container):\n",
    "        # If provided, `embeddings_container` is expected to be a URI to folder, the folder can be empty.\n",
    "        # Each sub-folder is generated by a `create_embeddings_component` run and can be reused for subsequent embeddings runs.\n",
    "        generate_embeddings.outputs.embeddings = Output(\n",
    "            type=\"uri_folder\", path=f\"{embeddings_container.path}/{{name}}\"\n",
    "        )\n",
    "\n",
    "    create_faiss_index = create_faiss_index_component(\n",
    "        embeddings=generate_embeddings.outputs.embeddings,\n",
    "    )\n",
    "    use_automatic_compute(create_faiss_index)\n",
    "\n",
    "    register_mlindex = register_mlindex_component(\n",
    "        storage_uri=create_faiss_index.outputs.index, asset_name=asset_name\n",
    "    )\n",
    "    use_automatic_compute(register_mlindex)\n",
    "    return {\n",
    "        \"mlindex_asset_uri\": create_faiss_index.outputs.index,\n",
    "        \"mlindex_asset_id\": register_mlindex.outputs.asset_id,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the Pipeline Job by calling the `@pipeline` annotated function and providing input arguments.\n",
    "`asset_name` will be used when registering the MLIndex Data Asset produced by the `register_mlindex` component in the pipeline. This is how you can refer to the MLIndex within AzureML.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_name = \"dental_faiss_mlindex\"\n",
    "data_source_glob = \"**/contoso-dental.xls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "pipeline_job = local_to_faiss(\n",
    "    input_data=Input(\n",
    "        type=AssetTypes.URI_FOLDER, path=\"../data/\"\n",
    "    ),  # This will upload the data folder to the default Workspace Datastore `workspaceblobstore`\n",
    "    data_source_glob=data_source_glob,\n",
    "    data_source_url=my_data.path,\n",
    "    # Each run will save latest Embeddings to subfolder under this path, runs will load latest embeddings from container and reuse any unchanged chunk embeddings.\n",
    "    embeddings_model=embeddings_model_uri,\n",
    "    aoai_connection_id=aoai_connection_id,\n",
    "    embeddings_container=Input(\n",
    "        type=\"uri_folder\",\n",
    "        path=f\"azureml://datastores/workspaceblobstore/paths/embeddings/{asset_name}\",\n",
    "    ),\n",
    "    # Name of asset to register MLIndex under\n",
    "    asset_name=asset_name,\n",
    ")\n",
    "\n",
    "# By default AzureML Pipelines will reuse the output of previous component Runs when inputs have not changed.\n",
    "# If you want to rerun the Pipeline every time each time so that any changes to upstream data sources are processed uncomment the below line.\n",
    "# pipeline_job.settings.force_rerun = True # Rerun each time so that git_clone isn't cached, if intent is to ingest latest data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add some properties to `pipeline_job` which ensure the Index generation progress and final Artifact appear in the PromptFlow Vector Index UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are added so that in progress index generations can be listed in UI, this tagging is done automatically by UI.\n",
    "pipeline_job.properties[\"azureml.mlIndexAssetName\"] = asset_name\n",
    "pipeline_job.properties[\"azureml.mlIndexAssetKind\"] = \"faiss\"\n",
    "pipeline_job.properties[\"azureml.mlIndexAssetSource\"] = \"Uri Folder\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Pipeline\n",
    "\n",
    "The output of each step in the pipeline can be inspected via the Workspace UI, click the link under 'Details Page' after running the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "\u001b[32mUploading data (0.29 MBs): 100%|██████████| 289399/289399 [00:01<00:00, 255060.14it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>local_to_faiss</td><td>lucid_eagle_ks5rjrzvhy</td><td>pipeline</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/lucid_eagle_ks5rjrzvhy?wsid=/subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourcegroups/labuser202_rg/workspaces/azmli&amp;tid=8c86751c-43a4-49e9-a39b-a6efb12feab1\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f3d014521c0>, 'embeddings_model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f3d014521f0>, 'asset_name': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f3d01452220>, 'chunk_size': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f3d01452250>, 'data_source_glob': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f3d01452280>, 'data_source_url': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f3d014522b0>, 'aoai_connection_id': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f3d014522e0>, 'embeddings_container': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f3d01452310>}, 'outputs': {'mlindex_asset_uri': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f3d01452340>, 'mlindex_asset_id': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f3d01452370>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': 'Pipeline to generate embeddings for a `input_data` source and create a Faiss index.', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/workspaces/rai-prompt-flow-workshop/src', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f3d014b5a90>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'local_to_faiss', 'is_deterministic': None, 'inputs': {'input_data': {}, 'embeddings_model': {}, 'asset_name': {}, 'chunk_size': {}, 'data_source_glob': {}, 'data_source_url': {}, 'aoai_connection_id': {}, 'embeddings_container': {}}, 'outputs': {'mlindex_asset_uri': {}, 'mlindex_asset_id': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'crack_and_chunk': Command({'parameters': {}, 'init': False, 'name': 'crack_and_chunk', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/workspaces/rai-prompt-flow-workshop/src', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f3d3059a070>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'input_data': '${{parent.inputs.input_data}}', 'input_glob': '${{parent.inputs.data_source_glob}}', 'document_path_replacement_regex': '${{parent.inputs.document_path_replacement_regex}}'}, 'job_outputs': {}, 'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f3d014a86d0>, 'input_glob': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f3d014a8100>, 'document_path_replacement_regex': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f3d014a8670>}, 'outputs': {}, 'component': 'azureml://registries/azureml/components/llm_rag_crack_and_chunk/versions/0.0.55', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'df6f580b-1500-40f5-ad54-f6afef654c5e', 'source': 'REMOTE.REGISTRY', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': {'instance_count': 1, 'instance_type': 'Standard_DS12_v2'}, 'queue_settings': None, 'swept': False}), 'generate_embeddings': Command({'parameters': {}, 'init': False, 'name': 'generate_embeddings', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/workspaces/rai-prompt-flow-workshop/src', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f3d305c44f0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'embeddings_model': 'azure_open_ai://deployment/text-embedding-ada-002/model/text-embedding-ada-002', 'chunks_source': '${{parent.jobs.crack_and_chunk.outputs.output_chunks}}'}, 'job_outputs': {'embeddings': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceblobstore/paths/embeddings/dental_faiss_mlindex/{name}'}}, 'inputs': {'embeddings_model': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f3d30589cd0>, 'chunks_source': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f3d30589520>}, 'outputs': {'embeddings': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f3d30589c70>}, 'component': 'azureml://registries/azureml/components/llm_rag_generate_embeddings/versions/0.0.48', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '3c66093c-b11b-4e73-9730-36cc9615078a', 'source': 'REMOTE.REGISTRY', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {'AZUREML_WORKSPACE_CONNECTION_ID_AOAI': '${{parent.inputs.aoai_connection_id}}'}, 'environment': None, 'resources': {'instance_count': 1, 'instance_type': 'Standard_DS12_v2'}, 'queue_settings': None, 'swept': False}), 'create_faiss_index': Command({'parameters': {}, 'init': False, 'name': 'create_faiss_index', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/workspaces/rai-prompt-flow-workshop/src', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f3d014b5970>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'embeddings': '${{parent.jobs.generate_embeddings.outputs.embeddings}}'}, 'job_outputs': {'index': '${{parent.outputs.mlindex_asset_uri}}'}, 'inputs': {'embeddings': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f3d014b5e20>}, 'outputs': {'index': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f3d014b5be0>}, 'component': 'azureml://registries/azureml/components/llm_rag_create_faiss_index/versions/0.0.53', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '8cd19a62-c16c-4b22-9036-3c5bf39cd5c6', 'source': 'REMOTE.REGISTRY', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': {'instance_count': 1, 'instance_type': 'Standard_DS12_v2'}, 'queue_settings': None, 'swept': False}), 'register_mlindex': Command({'parameters': {}, 'init': False, 'name': 'register_mlindex', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/workspaces/rai-prompt-flow-workshop/src', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f3d014b5460>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'storage_uri': '${{parent.jobs.create_faiss_index.outputs.index}}', 'asset_name': '${{parent.inputs.asset_name}}'}, 'job_outputs': {'asset_id': '${{parent.outputs.mlindex_asset_id}}'}, 'inputs': {'storage_uri': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f3d014b5580>, 'asset_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f3d014b5a00>}, 'outputs': {'asset_id': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f3d014b5040>}, 'component': 'azureml://registries/azureml/components/llm_rag_register_mlindex_asset/versions/0.0.52', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'e7360bd7-3db7-4c8f-8fad-a99bd0fe3be8', 'source': 'REMOTE.REGISTRY', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': {'instance_count': 1, 'instance_type': 'Standard_DS12_v2'}, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 4}, 'job_sources': {'REMOTE.REGISTRY': 4}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'NotStarted', 'log_files': None, 'name': 'lucid_eagle_ks5rjrzvhy', 'description': 'Pipeline to generate embeddings for a `input_data` source and create a Faiss index.', 'tags': {}, 'properties': {'azureml.mlIndexAssetName': 'dental_faiss_mlindex', 'azureml.mlIndexAssetKind': 'faiss', 'azureml.mlIndexAssetSource': 'Uri Folder', 'mlflow.source.git.repoURL': 'https://github.com/Azure-Samples/rai-prompt-flow-workshop', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': '96f074fe4dc7f3e47b9c0d39fc2fe4a8696b759c', 'azureml.git.dirty': 'True'}, 'print_as_yaml': True, 'id': '/subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourceGroups/labuser202_rg/providers/Microsoft.MachineLearningServices/workspaces/azmli/jobs/lucid_eagle_ks5rjrzvhy', 'Resource__source_path': None, 'base_path': '/workspaces/rai-prompt-flow-workshop/src', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f3d014520d0>, 'serialize': <msrest.serialization.Serializer object at 0x7f3d014523d0>, 'display_name': 'local_to_faiss', 'experiment_name': 'local_to_faiss', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://japaneast.api.azureml.ms/mlflow/v1.0/subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourceGroups/labuser202_rg/providers/Microsoft.MachineLearningServices/workspaces/azmli?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/lucid_eagle_ks5rjrzvhy?wsid=/subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourcegroups/labuser202_rg/workspaces/azmli&tid=8c86751c-43a4-49e9-a39b-a6efb12feab1', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"local_to_faiss\"\n",
    ")\n",
    "running_pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: lucid_eagle_ks5rjrzvhy\n",
      "Web View: https://ml.azure.com/runs/lucid_eagle_ks5rjrzvhy?wsid=/subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourcegroups/labuser202_rg/workspaces/azmli\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-02-23 23:30:34Z] Submitting 1 runs, first five are: 2c699b5e:10375767-fba5-4bcf-8ac0-9f8355e59297\n",
      "[2024-02-23 23:36:49Z] Completing processing run id 10375767-fba5-4bcf-8ac0-9f8355e59297.\n",
      "[2024-02-23 23:36:50Z] Submitting 1 runs, first five are: d20c67fe:d010e885-059c-4049-8b33-aafbe77cbfff\n",
      "[2024-02-23 23:38:44Z] Completing processing run id d010e885-059c-4049-8b33-aafbe77cbfff.\n",
      "[2024-02-23 23:38:45Z] Submitting 1 runs, first five are: d043d426:69ba3451-1bd4-4938-89a7-fb26b24ddaf3\n",
      "[2024-02-23 23:39:38Z] Completing processing run id 69ba3451-1bd4-4938-89a7-fb26b24ddaf3.\n",
      "[2024-02-23 23:39:38Z] Submitting 1 runs, first five are: 494aed28:09cab55e-ef16-45ed-9cb9-4f77d93cb191\n",
      "[2024-02-23 23:40:29Z] Completing processing run id 09cab55e-ef16-45ed-9cb9-4f77d93cb191.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: lucid_eagle_ks5rjrzvhy\n",
      "Web View: https://ml.azure.com/runs/lucid_eagle_ks5rjrzvhy?wsid=/subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourcegroups/labuser202_rg/workspaces/azmli\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_client.jobs.stream(running_pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use MLIndex with PromptFlow\n",
    "\n",
    "To use the MLindex in PromptFlow the asset_id can be used with the `Vector Index Lookup​` Tool. Replace `versions/2` with `versions/latest` to use the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml://subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourceGroups/labuser202_rg/providers/Microsoft.MachineLearningServices/workspaces/azmli/data/dental_faiss_mlindex/versions/1\n"
     ]
    }
   ],
   "source": [
    "asset_id = f\"azureml:/{ml_client.data.get(asset_name, label='latest').id}\"\n",
    "print(asset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml://subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourceGroups/labuser202_rg/providers/Microsoft.MachineLearningServices/workspaces/azmli/data/dental_faiss_mlindex/versions/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'azureml://subscriptions/7cfa902c-3eba-40b9-9d87-83369ec9e86d/resourcegroups/labuser202_rg/providers/Microsoft.MachineLearningServices/workspaces/azmli/data/dental_faiss_mlindex/versions/1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_id = f\"azureml:/{ml_client.data.get(asset_name, label='latest').id}\"\n",
    "print(asset_id)\n",
    "\n",
    "asset_id.replace(\"resourceGroups\", \"resourcegroups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
